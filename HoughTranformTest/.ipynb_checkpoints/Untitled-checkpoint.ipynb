{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Solution\n",
    "posted here: https://stackoverflow.com/questions/27969091/processing-an-image-of-a-table-to-get-data-from-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# the list of images (tables)\n",
    "images = ['table1.png', 'table2.png', 'table3.png', 'table4.png', 'table5.png']\n",
    "\n",
    "# the list of templates (used for template matching)\n",
    "templates = ['train1.png']\n",
    "\n",
    "def remove_duplicates(lines):\n",
    "    # remove duplicate lines (lines within 10 pixels of eachother)\n",
    "    for x1, y1, x2, y2 in lines:\n",
    "        for index, (x3, y3, x4, y4) in enumerate(lines):\n",
    "            if y1 == y2 and y3 == y4:\n",
    "                diff = abs(y1-y3)\n",
    "            elif x1 == x2 and x3 == x4:\n",
    "                diff = abs(x1-x3)\n",
    "            else:\n",
    "                diff = 0\n",
    "            if diff < 10 and diff is not 0:\n",
    "                del lines[index]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def sort_line_list(lines):\n",
    "    # sort lines into horizontal and vertical\n",
    "    vertical = []\n",
    "    horizontal = []\n",
    "    for line in lines:\n",
    "        if line[0] == line[2]:\n",
    "            vertical.append(line)\n",
    "        elif line[1] == line[3]:\n",
    "            horizontal.append(line)\n",
    "    vertical.sort()\n",
    "    horizontal.sort(key=lambda x: x[1])\n",
    "    return horizontal, vertical\n",
    "\n",
    "\n",
    "def hough_transform_p(image, template, tableCnt):\n",
    "    # open and process images\n",
    "    img = cv2.imread('imgs/'+image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "    # probabilistic hough transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 200, minLineLength=20, maxLineGap=999)[0].tolist()\n",
    "\n",
    "    # remove duplicates\n",
    "    lines = remove_duplicates(lines)\n",
    "\n",
    "    # draw image\n",
    "    for x1, y1, x2, y2 in lines:\n",
    "        cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "    # sort lines into vertical & horizontal lists\n",
    "    horizontal, vertical = sort_line_list(lines)\n",
    "\n",
    "    # go through each horizontal line (aka row)\n",
    "    rows = []\n",
    "    for i, h in enumerate(horizontal):\n",
    "        if i < len(horizontal)-1:\n",
    "            row = []\n",
    "            for j, v in enumerate(vertical):\n",
    "                if i < len(horizontal)-1 and j < len(vertical)-1:\n",
    "                    # every cell before last cell\n",
    "                    # get width & height\n",
    "                    width = horizontal[i+1][1] - h[1]\n",
    "                    height = vertical[j+1][0] - v[0]\n",
    "\n",
    "                else:\n",
    "                    # last cell, width = cell start to end of image\n",
    "                    # get width & height\n",
    "                    width = tW\n",
    "                    height = tH\n",
    "                tW = width\n",
    "                tH = height\n",
    "\n",
    "                # get roi (region of interest) to find an x\n",
    "                roi = img[h[1]:h[1]+width, v[0]:v[0]+height]\n",
    "\n",
    "                # save image (for testing)\n",
    "                dir = 'imgs/table%s' % (tableCnt+1)\n",
    "                if not os.path.exists(dir):\n",
    "                     os.makedirs(dir)\n",
    "                fn = '%s/roi_r%s-c%s.png' % (dir, i, j)\n",
    "                cv2.imwrite(fn, roi)\n",
    "\n",
    "                # if roi contains an x, add x to array, else add _\n",
    "                roi_gry = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "                ret, thresh = cv2.threshold(roi_gry, 127, 255, 0)\n",
    "                contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                if len(contours) > 1:\n",
    "                    # there is an x for 2 or more contours\n",
    "                    row.append('x')\n",
    "                else:\n",
    "                    # there is no x when len(contours) is <= 1\n",
    "                    row.append('_')\n",
    "            row.pop()\n",
    "            rows.append(row)\n",
    "\n",
    "    # save image (for testing)\n",
    "    fn = os.path.splitext(image)[0] + '-hough_p.png'\n",
    "    cv2.imwrite('imgs/'+fn, img)\n",
    "\n",
    "\n",
    "def process():\n",
    "    for i, img in enumerate(images):\n",
    "        # perform probabilistic hough transform on each image\n",
    "        hough_transform_p(img, templates[0], i)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Playing around with the Hough Transformation\n",
    "From the python cv2 page: http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html?highlight=detect%20lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('images/sudoku-original.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "\n",
    "lines = cv2.HoughLines(edges, 1, np.pi/180, 1)\n",
    "for rho,theta in lines[0]:\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "cv2.imwrite('images/sudoku-original-gray.jpg', gray)\n",
    "cv2.imwrite('images/sudoku-original-edges.jpg', edges)\n",
    "cv2.imwrite('images/houghlines1.jpg', img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/sudoku-original-edges.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/houghlines1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Playing around with the Hough Transformation 2\n",
    "From this stack overflow question: https://stackoverflow.com/questions/45322630/how-to-detect-lines-in-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/voltest-original.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "\n",
    "rho = 1  # distance resolution in pixels of the Hough grid (originally 1)\n",
    "theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "threshold = 200  # minimum number of votes (intersections in Hough grid cell) (originally 15)\n",
    "min_line_length = 55  # minimum number of pixels making up a line (originally 50)\n",
    "max_line_gap = 1  # maximum gap in pixels between connectable line segments (originally 20)\n",
    "line_image = np.copy(img) * 0  # creating a blank to draw lines on\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "# Output \"lines\" is an array containing endpoints of detected line segments\n",
    "lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n",
    "                    min_line_length, max_line_gap)\n",
    "\n",
    "#for line in lines:\n",
    "#    for x1,y1,x2,y2 in line:\n",
    "#        cv2.line(line_image, (x1,y1), (x2,y2), (255,0,0), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw the lines on the  image\n",
    "lines_edges = cv2.addWeighted(img, 0.8, line_image, 1, 0)\n",
    "cv2.imwrite('images/voltest-lines.jpg', lines_edges)\n",
    "cv2.imwrite('images/voltest-lines-separate.jpg', line_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best Hough Transformation\n",
    "Below is my approach to tuning parameters and then applying the hough transformation to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(img, lines, params):\n",
    "    \n",
    "    # Create a blank image to draw lines on\n",
    "    line_image = np.copy(img) * 0  \n",
    "    \n",
    "    # Draw lines from hough transform\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image, (x1,y1), (x2,y2), (255,0,0), 5)\n",
    "    \n",
    "    # Output image to file\n",
    "    filename = 'images/voltest3-lines-{}-{}-{}.jpg'.format(params[0], params[1], params[2]) \n",
    "    cv2.imwrite(filename, line_image)\n",
    "    \n",
    "def hough_transform(img, threshold, min_line_length, max_line_gap):\n",
    "    \n",
    "    # Convert to grayscale and detect edges\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray,50,150,apertureSize = 3) \n",
    "    \n",
    "    rho = 1  # distance resolution in pixels of the Hough grid (originally 1)\n",
    "    theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "    #threshold = 200  # minimum number of votes (intersections in Hough grid cell) (originally 15)\n",
    "    #min_line_length = 55  # minimum number of pixels making up a line (originally 50)\n",
    "    #max_line_gap = 1  # maximum gap in pixels between connectable line segments (originally 20)\n",
    "    params = [threshold, min_line_length, max_line_gap] # for output to draw_image\n",
    "    \n",
    "    # Apply Hough Transformation\n",
    "    lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n",
    "                    min_line_length, max_line_gap)\n",
    "\n",
    "    # Draw Image & Output\n",
    "    draw_image(img, lines, params)\n",
    "\n",
    "# Load original image\n",
    "img = cv2.imread('images/voltest3.png')\n",
    "\n",
    "# Tuning parameters\n",
    "thresholds = np.arange(200, 300, 100)\n",
    "min_line_lengths = np.arange(10, 26, 25)\n",
    "max_line_gaps = np.arange(0,15,5)\n",
    "\n",
    "# Run hough transform with all parameters\n",
    "for threshold_val in thresholds:\n",
    "    for minline_val in min_line_lengths:\n",
    "        for maxgap_val in max_line_gaps:\n",
    "            hough_transform(img, threshold_val, minline_val, maxgap_val)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider countour detection with hough transform, and template matching\n",
    "\n",
    "Helpful Links:\n",
    "https://stackoverflow.com/questions/10196198/how-to-remove-convexity-defects-in-a-sudoku-square\n",
    "\n",
    "https://stackoverflow.com/questions/27969091/processing-an-image-of-a-table-to-get-data-from-it\n",
    "\n",
    "https://www.google.com/search?q=opencv+dave.jpg&rlz=1C1SQJL_enUS762US762&tbm=isch&tbo=u&source=univ&sa=X&ved=2ahUKEwiclLHf3d3cAhVowFQKHd7-BmwQsAR6BAgFEAE&biw=2000&bih=958#imgrc=VS-1FritL6V2MM\n",
    "\n",
    "http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html?highlight=detect%20lines\n",
    "\n",
    "https://stackoverflow.com/questions/27969091/processing-an-image-of-a-table-to-get-data-from-it\n",
    "\n",
    "https://stackoverflow.com/questions/45322630/how-to-detect-lines-in-opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contour Detection\n",
    "Below is my approach to using contour detection (instead of the hough transformation) to detect the tables in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Prep image\n",
    "img = cv2.imread('images/voltest-original.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Run contour analysis\n",
    "ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours\n",
    "contours = sorted(contours, key=cv2. contourArea, reverse = True)\n",
    "manual_tbl_countours = contours[1:4]\n",
    "\n",
    "# next two lines were pulled from stack question\n",
    "perimeters = [cv2.arcLength(contours[i],True) for i in range(len(contours))]\n",
    "listindex=[i for i in range(15) if perimeters[i]>perimeters[0]/2]\n",
    "\n",
    "# from here: https://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/\n",
    "c = contours[1]\n",
    "(x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "# Show image\n",
    "imgcont = img.copy()\n",
    "#[cv2.drawContours(imgcont, [contours[i]], 0, (0,255,0), 5) for i in listindex]\n",
    "#plt.imshow(imgcont)\n",
    "cv2.drawContours(imgcont, contours, 10, (0,255,0), 3)\n",
    "cv2.imwrite(\"filename.jpg\", imgcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here: https://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/\n",
    "c = contours[2]\n",
    "(x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "print(x,y,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first, get sort the contours by size. 4 contours with largest area.\n",
    "2. second, sort by left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "931\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "931\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "horizontal\n",
      "935\n",
      "vertical\n",
      "-606\n",
      "vertical\n",
      "-613\n",
      "vertical\n",
      "-613\n",
      "vertical\n",
      "-606\n",
      "vertical\n",
      "-605\n",
      "vertical\n",
      "-605\n",
      "vertical\n",
      "-606\n",
      "vertical\n",
      "-613\n",
      "vertical\n",
      "-605\n",
      "vertical\n",
      "-605\n",
      "vertical\n",
      "-613\n",
      "vertical\n",
      "-606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From here: https://stackoverflow.com/questions/28759253/how-to-crop-the-internal-area-of-a-contour\n",
    "\n",
    "def remove_duplicates(lines):\n",
    "    # remove duplicate lines (lines within 10 pixels of eachother)\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "        #for x1, y1, x2, y2 in lines:\n",
    "            for index, (x3, y3, x4, y4) in enumerate(lines):\n",
    "                if y1 == y2 and y3 == y4:\n",
    "                    diff = abs(y1-y3)\n",
    "                elif x1 == x2 and x3 == x4:\n",
    "                    diff = abs(x1-x3)\n",
    "                else:\n",
    "                    diff = 0\n",
    "                if diff < 10 and diff is not 0:\n",
    "                    del lines[index]\n",
    "        return lines\n",
    "\n",
    "def sort_line_list(lines):\n",
    "    # sort lines into horizontal and vertical\n",
    "    vertical = []\n",
    "    horizontal = []\n",
    "    for line in lines:\n",
    "        if line[0] == line[2]:\n",
    "            vertical.append(line)\n",
    "        elif line[1] == line[3]:\n",
    "            horizontal.append(line)\n",
    "    vertical.sort()\n",
    "    horizontal.sort(key=lambda x: x[1])\n",
    "    return horizontal, vertical\n",
    "\n",
    "# Below script gets the table outline for contour index 3\n",
    "idx = 3\n",
    "mask = np.zeros_like(img) # Create mask where white is what we want, black otherwise\n",
    "cv2.drawContours(mask, contours, idx, (255,255,255), -1) # Draw filled contour in mask\n",
    "out = np.zeros_like(img) # Extract out the object and place into output image\n",
    "out[mask == 255] = img[mask == 255]\n",
    "# Crop masked image\n",
    "#(x, y) = np.where(mask == 255)\n",
    "#print(np.where(mask == 255))\n",
    "#(topx, topy) = (np.min(x), np.min(y))\n",
    "#(bottomx, bottomy) = (np.max(x), np.max(y))\n",
    "#out = out[topx:bottomx+1, topy:bottomy+1]\n",
    "\n",
    "# Convert to grayscale and detect edges\n",
    "gray = cv2.cvtColor(out, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 50, 150, apertureSize = 3)\n",
    "\n",
    "# Apply Hough Transformation\n",
    "lines = cv2.HoughLinesP(edges, rho = 1, theta = np.pi / 180, threshold = 200, minLineLength = 20, maxLineGap = 50)\n",
    "\n",
    "# Create a blank image to draw lines on\n",
    "line_image = np.copy(img) * 0  \n",
    "\n",
    "# Draw lines from hough transform\n",
    "vertical = []\n",
    "horizontal = []\n",
    "for line in lines:\n",
    "    for x1, y1, x2, y2 in line:\n",
    "        cv2.line(out, (x1,y1), (x2,y2), (255,255,255), 5)\n",
    "        if x1 == x2:\n",
    "            vertical.append(line)\n",
    "            print(\"vertical\")\n",
    "            print(y2 - y1)\n",
    "        elif y1 == y2:\n",
    "            horizontal.append(line)\n",
    "            print(\"horizontal\")\n",
    "            print(x2 - x1)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite(\"output.jpg\", out)      \n",
    "# sort them\n",
    "#vertical = np.sort(vertical, axis=0)\n",
    "#print(vertical)\n",
    "#horizontal = np.sort(horizontal, axis = 1)\n",
    "\n",
    "# remove duplicates\n",
    "#lines = remove_duplicates(lines)            \n",
    "\n",
    "# sort lines into vertical & horizontal lists\n",
    "#horizontal, vertical = sort_line_list(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use my function defined above\n",
    "hough_transform(out, 200, 20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14', '20', '10', '30', '36', '22', '1061', '1071', '535', '889', '815', '382', '116', '98', '48', '74', '64', '45', '1191', '1189', '593', '993', '915', '449']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import Image\n",
    "except ImportError:\n",
    "    from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# If you don't have tesseract executable in your PATH, include the following:\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract'\n",
    "\n",
    "# Define config parameters.\n",
    "# '-l eng'  for using the English language\n",
    "# '--oem 1' for using LSTM OCR Engine\n",
    "config = ('-l eng --oem 1 --psm 3')\n",
    " \n",
    "# Read image from disk\n",
    "#im = cv2.imread(imPath, cv2.IMREAD_COLOR)\n",
    "img_crop = cv2.imread('output_cropped.jpg')\n",
    "# Run tesseract OCR on image\n",
    "text = pytesseract.image_to_string(img_crop, config=config)\n",
    "\n",
    "counts = text.split()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Workflow:\n",
    "\n",
    "1. Detect large boxes using contours\n",
    "2. Crop image (function)\n",
    "3. Using hough transformation, detect table lines (function)\n",
    "4. Recolor table lines to white, crop image\n",
    "5. Run tesseract on resulting table, extract numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'xing_leg': 'SL', 'type': 'Ped', 'volume': 73}, {'xing_leg': 'SL', 'type': 'Sch', 'volume': 29}, {'xing_leg': 'NL', 'type': 'Ped', 'volume': 0}, {'xing_leg': 'NL', 'type': 'Sch', 'volume': 0}, {'xing_leg': 'WL', 'type': 'Ped', 'volume': 32}, {'xing_leg': 'WL', 'type': 'Sch', 'volume': 1}, {'xing_leg': 'EL', 'type': 'Ped', 'volume': 35}, {'xing_leg': 'EL', 'type': 'Sch', 'volume': 9}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "from operator import itemgetter, attrgetter\n",
    "try:\n",
    "    import Image\n",
    "except ImportError:\n",
    "    from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Set path to tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract'\n",
    "# Define config parameters.\n",
    "# '-l eng'  for using the English language\n",
    "# '--oem 1' for using LSTM OCR Engine\n",
    "config = ('-l eng --oem 0 --psm 10000 -c tessedit_char_whitelist=0123456789')\n",
    "\n",
    "def GetContours(img):\n",
    "    # Prep image\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "    # Run contour analysis, sort by contour area (descending)\n",
    "    im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse = True)\n",
    "    return(contours)\n",
    "\n",
    "def CropImage(img, contour):\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "    crop_img = img[y:y+h, x:x+w]\n",
    "    return(crop_img)\n",
    "\n",
    "def TesseractText(img):\n",
    "    text = pytesseract.image_to_string(img, config=config)\n",
    "    counts = list(map(int, text.split()))\n",
    "    # hmm, maybe here i shoudl be concatenating everything, instead of eventually\n",
    "    # only returning the first object in the list\n",
    "    return(counts)\n",
    "\n",
    "def ExtractCellVal(cells, img):\n",
    "    vol = []\n",
    "    # for each cell, crop & extract text\n",
    "    for cell in cells:\n",
    "        (x, y, w, h) = cell[1], cell[2], cell[3], cell[4]\n",
    "        crop_img = img[y:y+h, x:x+w]\n",
    "        val = TesseractText(crop_img)\n",
    "        vol.append(val[0])\n",
    "    return(vol)\n",
    "    \n",
    "def SortPedCells(contours):\n",
    "    # Get the bounding box of each contour\n",
    "    contour_list = []\n",
    "    contour_len = len(contours)\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        contour_list.append([contour, x, y, w, h])\n",
    "    contour_a = np.array(contour_list)\n",
    "    # Sort by x coordinate, split by number of columns (in this case, 2)\n",
    "    contour_a = contour_a[contour_a[:,1].argsort()]\n",
    "    pedvol = contour_a[:6]\n",
    "    schvol = contour_a[6:]\n",
    "    # Sort top to bottom (descending) by y coordinate\n",
    "    pedvol = pedvol[pedvol[:,2].argsort()]\n",
    "    schvol = schvol[schvol[:,2].argsort()]\n",
    "    return(pedvol, schvol)\n",
    "    \n",
    "def AnalyzePedCrossingTable(img, pedtbl_contour):\n",
    "    # Crop Image, get new contours\n",
    "    crop_img = CropImage(img, pedtbl_contour[0])\n",
    "    pedvol_contours = GetContours(crop_img)\n",
    "    pedvol_cells = pedvol_contours[2:14]\n",
    "    pedvol_cells, schvol_cells = SortPedCells(pedvol_cells)\n",
    "    pedvol = ExtractCellVal(pedvol_cells, crop_img)\n",
    "    schvol = ExtractCellVal(schvol_cells, crop_img)\n",
    "    return(dict([(\"Ped\", pedvol), (\"Sch\", schvol)]))\n",
    "    \n",
    "def GetPedData(img):\n",
    "    ped_tbl_contours = GetContours(img)[5:9]\n",
    "    ped_tbls = []\n",
    "    for ped_tbl_contour in ped_tbl_contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(ped_tbl_contour)\n",
    "        ped_tbls.append([ped_tbl_contour, x, y, w, h])\n",
    "    ped_tbls = np.array(ped_tbls)\n",
    "    ped_tbls = sorted(ped_tbls, key=itemgetter(1))\n",
    "    ped_tbls = sorted(ped_tbls, key=itemgetter(2))\n",
    "    \n",
    "    ped_sch_extract = {}\n",
    "    ped_sch_extract['SL'] = AnalyzePedCrossingTable(img, ped_tbls[:1][0])\n",
    "    ped_sch_extract['NL'] = AnalyzePedCrossingTable(img, ped_tbls[1:2][0])\n",
    "    ped_sch_extract['WL'] = AnalyzePedCrossingTable(img, ped_tbls[2:3][0])\n",
    "    ped_sch_extract['EL'] = AnalyzePedCrossingTable(img, ped_tbls[3:4][0])\n",
    "    \n",
    "    # Format as final df\n",
    "    ped_sch_data = []\n",
    "    for leg in ped_sch_extract:\n",
    "        for pedtype in ped_sch_extract[leg]:\n",
    "\n",
    "            ped_sch_dict = {}\n",
    "            ped_sch_dict['xing_leg'] = leg\n",
    "            ped_sch_dict['type'] = pedtype\n",
    "            ped_sch_dict['volume'] = sum(ped_sch_extract[leg][pedtype])\n",
    "            ped_sch_data.append(ped_sch_dict)\n",
    "    \n",
    "    return(ped_sch_data)\n",
    "\n",
    "img = cv2.imread('images2/1ST.FRESNO.160608-MAN.png')\n",
    "ManualTC['Pedestrian'] = GetPedData(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert PDF Images to PNG for Processing with CV2\n",
    "Before doing conversions from PDF to PNG, make sure to install ImageMagick (along with wand) as well as Ghostscript. Add more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.image import Image, Color\n",
    "import glob, os\n",
    "\n",
    "# Grab all PDFs within the folder\n",
    "files = glob.glob('C:/Users/Tim/Documents/GitHub/vehicle-vol-pdf-scrape/HoughTranformTest/images2/*.pdf')\n",
    "file_names = [os.path.abspath(file) for file in files]\n",
    "\n",
    "for file_name in file_names:\n",
    "    fin, file_extension = os.path.splitext(file_name)\n",
    "    fout = fin + '.png'\n",
    "\n",
    "    with Image(filename=file_name, resolution=300) as img:\n",
    "        img.background_color = Color(\"white\")\n",
    "        img.alpha_channel = 'remove'\n",
    "        img.save(filename=fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Stuctural Similarity Index (SSIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.ALAMEDA.140409LATESHIFT-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.ALAMEDAEARLYSHIFT.140227-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.BEAUDRY.150203-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.CUMMINGS.120614-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.DACOTAH.120615-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.DACOTAH.160602-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.EVERGREEN.160519-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.FRESNO.160608-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-0.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-1.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-2.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-3.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-4.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HILL.160202-MAN-5.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.HOPE.151112-NDSMAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.PACIFIC.120726-MAN.png\n",
      "1.0\n",
      "C:\\Users\\Tim\\Documents\\GitHub\\vehicle-vol-pdf-scrape\\HoughTranformTest\\images2\\1ST.SPRING.130319-MAN.png\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_mse as mse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#hist1 = cv2.calcHist([image],[0],None,[256],[0,256])\n",
    "#hist2 = cv2.calcHist([image1],[0],None,[256],[0,256])\n",
    "#compare = cv2.compareHist(hist1,hist2,CV_COMP_CORREL)\n",
    "\n",
    "# Import images\n",
    "imageA = cv2.imread(\"images2/1ST.ALAMEDA.140409LATESHIFT-MAN.png\")\n",
    "#imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "histA = cv2.calcHist([imageA],[0],None,[256],[0,256])\n",
    "\n",
    "# Grab all PDFs within the folder\n",
    "files = glob.glob('C:/Users/Tim/Documents/GitHub/vehicle-vol-pdf-scrape/HoughTranformTest/images2/*.png')\n",
    "file_names = [os.path.abspath(file) for file in files]\n",
    "\n",
    "for file_name in file_names:\n",
    "    imageB = cv2.imread(file_name)\n",
    "    \n",
    "    # Print filename\n",
    "    print(file_name)\n",
    "\n",
    "    # convert the images to grayscale\n",
    "    #imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "    histB = cv2.calcHist([imageB],[0],None,[256],[0,256])\n",
    "\n",
    "    # compute SSIM\n",
    "    #s = ssim(imageA, imageB)\n",
    "    #m = mse(imageA, imageB)\n",
    "    #print(s, m)\n",
    "    compare = cv2.compareHist(histA, histB, 0)\n",
    "    print(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
