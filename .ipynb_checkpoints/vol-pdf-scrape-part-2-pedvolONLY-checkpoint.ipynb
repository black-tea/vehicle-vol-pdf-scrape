{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping LADOT Volume Data from PDFs, Part 2\n",
    "\n",
    "##### Where I Left Off\n",
    "In the first python notebook, I described the problem and general approach I was taking to solve it. I downloaded all the manual count PDF documents and setup the file tables for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Text Extraction Process\n",
    "As I discussed in the first python notebook, my process for extracting the text into usable data can be broken down into the few key parts: (1) define bounding boxes, (2) search for text within the bounding boxes (3) reformat the resulting text into multiple data tables, and (4) join the resulting tables to the ID established by the Bureau of Engineering. \n",
    "\n",
    "##### Define Bounding Boxes\n",
    "This was tricky. I initially began defining bounding boxes using pixel measurements from a few sample pages. However, I quickly realized that due to the second of the challenges I mentioned above that this would not work, since the tables are in different locations among the PDF documents. \n",
    "\n",
    "Instead, I decided to create bounding boxes on the fly for each document using relative positioning of certain keywords that appeared almost always on each PDF document. Using pdfquery, I could begin by searching the document for these keywords and then extract the x,y pixel coordinate locations for the bounding box of each one. By getting the coordinates of multiple keyword objects on the page, I could construct a set of bounding boxes that seemed to perform relatively well in capturing data tables.\n",
    "\n",
    "(create image of what this looked like)\n",
    "\n",
    "##### Search for Text within Bounding Boxes\n",
    "Once I had the coordinates of the bounding boxes, this part was quite easy, using PDFQuery to extract text\n",
    "\n",
    "(do i need to adjust any of the parameters in rapidminer??)\n",
    "\n",
    "##### Reformat the Resulting Text into Data Tables\n",
    "The final problem included taking the scraped text from the bounding boxes and reformatting them into usable data tables. I kept in mind the relational database model as I set the format for these tables. From the PDF image above, I decided on the following tables and attributes:\n",
    "\n",
    "*tbl_manualcount_info:* This table contains the basic information about the manual count summary. Each count will have one tuple with the following information:\n",
    "* street_ns: The North / South Street running through the intersection\n",
    "* street_ew: The East / West street running through the intersection\n",
    "* dayofweek: Day of the Week\n",
    "* date: Date, in datetime format\n",
    "* weather: Prevailing weather at the time of the count (Clear, Sunny, etc.)\n",
    "* hours: the hours of the count (text)\n",
    "* school_day: A Yes / No indication of whether the count occurred on a school day. This is important because it heavily affects the volume counts\n",
    "* int_code: The \"I/S Code\" on the form corresponds to the CL_Node_ID on the BOE Centerline. This ID field makes it easy to join to the City's centerline network\n",
    "* district: The DOT field district in which the count took place\n",
    "* count_id: Unique identifier assigned to the summary\n",
    "\n",
    "*tbl_manualcount_dualwheel:* This table contains count data for dual-wheeled (motorcycles), bikes, and buses. Each form will have 12 tuples with the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Intersection approach being measured (N,S,E,W)\n",
    "* type: Dual-Wheeled / Bikes / Buses\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_peak:* This table contains the peak hour / 15 minute counts. Each form will have 16 tuples with the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Intersection approach being measured (N,S,E,W)\n",
    "* type: The type of count\n",
    "    * am_15: The AM peak 15 minute count\n",
    "    * am_hour: The AM peak hour count\n",
    "    * pm_15: The PM peak 15 minute count\n",
    "    * pm_hour: The PM peak hour count\n",
    "* time: Time of each count (in datetime format)\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_volumes:* This table contains the main volume counts for each approach at the intersection. The number of tuples for each form will vary depending on the number of hours surveyed. A 6-hour count will have 6 hours * 3 directions (left, right, through) * 4 approach directions = 54 tuples. Each tuple will have the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Northbound (NB) / Southbound (SB) / Eastbound (EB) / Westbound (WB)\n",
    "* movement: Right-Turn (Rt) / Through (Th) / Left-Turn (Lt)\n",
    "* start_time: Start time of that count, in datetime format\n",
    "* end_time: End time of that count, in datetime format\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_peds:* This table contains pedestrian and schoolchildren counts during the same time as the main volume counts, so the number of tuples will also be dependent on the number of hours the location was surveyed. Each tuple will have the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* xing_leg: The leg of the intersection that is being crossed. South Leg (SL) / North Leg (NL) / West Leg (WL) / East Leg (EL)\n",
    "* type: Ped / Sch\n",
    "* start_time: Start time of that count, in datetime format\n",
    "* end_time: End time of that count, in datetime format\n",
    "* volume: Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26840 total count files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_node_id</th>\n",
       "      <th>location</th>\n",
       "      <th>traffic_id</th>\n",
       "      <th>type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3667</td>\n",
       "      <td>ISLAND AVE at L ST</td>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>3667_ISLLST94.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3680</td>\n",
       "      <td>FIGUEROA ST at L ST</td>\n",
       "      <td>2</td>\n",
       "      <td>manual</td>\n",
       "      <td>3680_FIGLST94.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3727</td>\n",
       "      <td>FRIES AVE at HARRY BRIDGES BLVD</td>\n",
       "      <td>3</td>\n",
       "      <td>manual</td>\n",
       "      <td>3727_FRIHAR95.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3787</td>\n",
       "      <td>ANAHEIM ST AT FARRAGUT AVE</td>\n",
       "      <td>4</td>\n",
       "      <td>manual</td>\n",
       "      <td>3787_ANAFAR100817.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3787</td>\n",
       "      <td>ANAHEIM ST AT FARRAGUT AVE</td>\n",
       "      <td>4</td>\n",
       "      <td>manual</td>\n",
       "      <td>FARRAGUT.ANAHEIM.110119-MAN.PDF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cl_node_id                         location  traffic_id    type  \\\n",
       "count_id                                                                   \n",
       "1              3667               ISLAND AVE at L ST           1  manual   \n",
       "2              3680              FIGUEROA ST at L ST           2  manual   \n",
       "3              3727  FRIES AVE at HARRY BRIDGES BLVD           3  manual   \n",
       "4              3787       ANAHEIM ST AT FARRAGUT AVE           4  manual   \n",
       "5              3787       ANAHEIM ST AT FARRAGUT AVE           4  manual   \n",
       "\n",
       "                                     file  \n",
       "count_id                                   \n",
       "1                       3667_ISLLST94.PDF  \n",
       "2                       3680_FIGLST94.PDF  \n",
       "3                       3727_FRIHAR95.PDF  \n",
       "4                   3787_ANAFAR100817.PDF  \n",
       "5         FARRAGUT.ANAHEIM.110119-MAN.PDF  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Setup\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime, date, time\n",
    "import pdfquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib2 import Path\n",
    "\n",
    "### Load NavLA Count file table\n",
    "count_files_df = pd.read_csv('data/TrafficCountFileStructure/navLAfiles.csv',index_col=0)\n",
    "print(\"There are {} total count files\".format(len(count_files_df)))\n",
    "count_files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the Data Tables\n",
    "Based on the outline above, the first step is to prepare dataframes for each of the volume-related tables discussed above. Once I have the dataframe table structures, the next step involves looping through all the manual count PDFs, running my text extract function, and then inserting the rows into the appropriate tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among the 26840 total count files, 9460, 35.2% are manual count files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_id</th>\n",
       "      <th>cl_node_id</th>\n",
       "      <th>location</th>\n",
       "      <th>traffic_id</th>\n",
       "      <th>type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3667</td>\n",
       "      <td>ISLAND AVE at L ST</td>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>3667_ISLLST94.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3680</td>\n",
       "      <td>FIGUEROA ST at L ST</td>\n",
       "      <td>2</td>\n",
       "      <td>manual</td>\n",
       "      <td>3680_FIGLST94.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3727</td>\n",
       "      <td>FRIES AVE at HARRY BRIDGES BLVD</td>\n",
       "      <td>3</td>\n",
       "      <td>manual</td>\n",
       "      <td>3727_FRIHAR95.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3787</td>\n",
       "      <td>ANAHEIM ST AT FARRAGUT AVE</td>\n",
       "      <td>4</td>\n",
       "      <td>manual</td>\n",
       "      <td>3787_ANAFAR100817.PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3787</td>\n",
       "      <td>ANAHEIM ST AT FARRAGUT AVE</td>\n",
       "      <td>4</td>\n",
       "      <td>manual</td>\n",
       "      <td>FARRAGUT.ANAHEIM.110119-MAN.PDF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_id cl_node_id                         location  traffic_id    type  \\\n",
       "0         1       3667               ISLAND AVE at L ST           1  manual   \n",
       "1         2       3680              FIGUEROA ST at L ST           2  manual   \n",
       "2         3       3727  FRIES AVE at HARRY BRIDGES BLVD           3  manual   \n",
       "3         4       3787       ANAHEIM ST AT FARRAGUT AVE           4  manual   \n",
       "4         5       3787       ANAHEIM ST AT FARRAGUT AVE           4  manual   \n",
       "\n",
       "                              file  \n",
       "0                3667_ISLLST94.PDF  \n",
       "1                3680_FIGLST94.PDF  \n",
       "2                3727_FRIHAR95.PDF  \n",
       "3            3787_ANAFAR100817.PDF  \n",
       "4  FARRAGUT.ANAHEIM.110119-MAN.PDF  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Load NavLA Count file table\n",
    "count_files_df = pd.read_csv('data/TrafficCountFileStructure/navLAfiles.csv',index_col=0)\n",
    "manual_count_files_df = count_files_df[(count_files_df['type'] == 'manual')].reset_index()\n",
    "print(\"Among the {} total count files, {}, {:.1%} are manual count files\".format(len(count_files_df), len(manual_count_files_df), (len(manual_count_files_df)/len(count_files_df))))\n",
    "manual_count_files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the PDF text scraping script (extract / transform)\n",
    "The powerhouse behind this process is the script I built to read a PDF and extract text into a list of dictionaries (one for each table). Because this script is so long, I opted not to include the code in the notebook, and instead treated it as a module that I imported using \"import VolumeCountSheets_V2\" above. I call the \"pdf_extract\" function from the module on each PDF.\n",
    "\n",
    "### Step 3: Append each resulting dictionary to the Pandas dataframe (load)\n",
    "The function returns a list of dictionaries, one for each dataframe. I then take the pandas dataframes I constructed in the cell above and append each dictionary to the appropriate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from operator import itemgetter, attrgetter\n",
    "try:\n",
    "    import Image\n",
    "except ImportError:\n",
    "    from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Set path to tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract'\n",
    "# Define config parameters.\n",
    "# '-l eng'  for using the English language\n",
    "# '--oem 1' for using LSTM OCR Engine\n",
    "config = ('-l eng --oem 0 --psm 10000 -c tessedit_char_whitelist=0123456789')\n",
    "\n",
    "def GetContours(img):\n",
    "    # Prep image\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "    # Run contour analysis, sort by contour area (descending)\n",
    "    im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse = True)\n",
    "    return(contours)\n",
    "\n",
    "def CropImage(img, contour):\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "    crop_img = img[y:y+h, x:x+w]\n",
    "    return(crop_img)\n",
    "\n",
    "def TesseractText(img):\n",
    "    text = pytesseract.image_to_string(img, config=config)\n",
    "    counts = list(map(int, text.split()))\n",
    "    # hmm, maybe here i shoudl be concatenating everything, instead of eventually\n",
    "    # only returning the first object in the list\n",
    "    return(counts)\n",
    "\n",
    "def ExtractCellVal(cells, img):\n",
    "    vol = []\n",
    "    # for each cell, crop & extract text\n",
    "    for cell in cells:\n",
    "        (x, y, w, h) = cell[1], cell[2], cell[3], cell[4]\n",
    "        crop_img = img[y:y+h, x:x+w]\n",
    "        val = TesseractText(crop_img)\n",
    "        vol.append(val[0])\n",
    "    return(vol)\n",
    "    \n",
    "def SortPedCells(contours):\n",
    "    # Get the bounding box of each contour\n",
    "    contour_list = []\n",
    "    contour_len = len(contours)\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        contour_list.append([contour, x, y, w, h])\n",
    "    contour_a = np.array(contour_list)\n",
    "    # Sort by x coordinate, split by number of columns (in this case, 2)\n",
    "    contour_a = contour_a[contour_a[:,1].argsort()]\n",
    "    pedvol = contour_a[:6]\n",
    "    schvol = contour_a[6:]\n",
    "    # Sort top to bottom (descending) by y coordinate\n",
    "    pedvol = pedvol[pedvol[:,2].argsort()]\n",
    "    schvol = schvol[schvol[:,2].argsort()]\n",
    "    return(pedvol, schvol)\n",
    "    \n",
    "def AnalyzePedCrossingTable(img, pedtbl_contour):\n",
    "    # Crop Image, get new contours\n",
    "    crop_img = CropImage(img, pedtbl_contour[0])\n",
    "    pedvol_contours = GetContours(crop_img)\n",
    "    pedvol_cells = pedvol_contours[2:14]\n",
    "    pedvol_cells, schvol_cells = SortPedCells(pedvol_cells)\n",
    "    pedvol = ExtractCellVal(pedvol_cells, crop_img)\n",
    "    schvol = ExtractCellVal(schvol_cells, crop_img)\n",
    "    return(dict([(\"Ped\", pedvol), (\"Sch\", schvol)]))\n",
    "    \n",
    "def GetPedData(img):\n",
    "    ped_tbl_contours = GetContours(img)[5:9]\n",
    "    ped_tbls = []\n",
    "    for ped_tbl_contour in ped_tbl_contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(ped_tbl_contour)\n",
    "        ped_tbls.append([ped_tbl_contour, x, y, w, h])\n",
    "    ped_tbls = np.array(ped_tbls)\n",
    "    ped_tbls = sorted(ped_tbls, key=itemgetter(1))\n",
    "    ped_tbls = sorted(ped_tbls, key=itemgetter(2))\n",
    "    \n",
    "    ped_sch_extract = {}\n",
    "    ped_sch_extract['SL'] = AnalyzePedCrossingTable(img, ped_tbls[:1][0])\n",
    "    ped_sch_extract['NL'] = AnalyzePedCrossingTable(img, ped_tbls[1:2][0])\n",
    "    ped_sch_extract['WL'] = AnalyzePedCrossingTable(img, ped_tbls[2:3][0])\n",
    "    ped_sch_extract['EL'] = AnalyzePedCrossingTable(img, ped_tbls[3:4][0])\n",
    "    \n",
    "    # Format as final df\n",
    "    ped_sch_data = []\n",
    "    for leg in ped_sch_extract:\n",
    "        for pedtype in ped_sch_extract[leg]:\n",
    "\n",
    "            ped_sch_dict = {}\n",
    "            ped_sch_dict['xing_leg'] = leg\n",
    "            ped_sch_dict['type'] = pedtype\n",
    "            ped_sch_dict['volume'] = sum(ped_sch_extract[leg][pedtype])\n",
    "            ped_sch_data.append(ped_sch_dict)\n",
    "    \n",
    "    return(ped_sch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Count:1\n",
      "7\n",
      "data/TrafficCountData/Manual/All/6TH.STANDREWS.120501-MAN.png\n",
      "multi-page pdf\n",
      "{'Pedestrian': [{'xing_leg': 'SL', 'type': 'Ped', 'volume': 233}, {'xing_leg': 'SL', 'type': 'Sch', 'volume': 12}, {'xing_leg': 'NL', 'type': 'Ped', 'volume': 215}, {'xing_leg': 'NL', 'type': 'Sch', 'volume': 8}, {'xing_leg': 'WL', 'type': 'Ped', 'volume': 164}, {'xing_leg': 'WL', 'type': 'Sch', 'volume': 13}, {'xing_leg': 'EL', 'type': 'Ped', 'volume': 214}, {'xing_leg': 'EL', 'type': 'Sch', 'volume': 29}]}\n",
      "  count_id type  volume xing_leg\n",
      "0        1  Ped     233       SL\n",
      "1        1  Sch      12       SL\n",
      "2        1  Ped     215       NL\n",
      "3        1  Sch       8       NL\n",
      "4        1  Ped     164       WL\n",
      "5        1  Sch      13       WL\n",
      "6        1  Ped     214       EL\n",
      "7        1  Sch      29       EL\n"
     ]
    }
   ],
   "source": [
    " %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pdfquery, sys, cv2\n",
    "from wand.image import Image, Color\n",
    "from wand.display import display\n",
    "\n",
    "# Setup Counter of sucessful / failed attempts\n",
    "success = 0\n",
    "failures = 0\n",
    "ped_volumes = []\n",
    "\n",
    "##### Loop through files, run function\n",
    "for index, row in manual_count_files_df.iterrows():\n",
    "    \n",
    "    # for testing purposes only\n",
    "    if index > 0:\n",
    "        break\n",
    "    \n",
    "    #print(row['file'])\n",
    "    #fileloc = 'data/TrafficCountData/Manual/All/' + row['file']\n",
    "    fileloc = 'data/TrafficCountData/Manual/All/6TH.STANDREWS.120501-MAN.pdf'\n",
    "    \n",
    "    # ID for the count\n",
    "    count_id = row['count_id']\n",
    "    if index%100 == 0:\n",
    "        print(\"Current Count:\" + str(index+1))\n",
    "        \n",
    "    if Path(fileloc).exists():\n",
    "        \n",
    "        try:\n",
    "            # Load the PDF\n",
    "            pdf = pdfquery.PDFQuery(fileloc,resort=False)\n",
    "            pdf.load()\n",
    "            search = pdf.pq('LTTextLineHorizontal:contains(\"MANUAL TRAFFIC COUNT SUMMARY\")')\n",
    "            \n",
    "            # If there is a keyword match, convert to image and run analysis\n",
    "            if len(search) > 0:\n",
    "                # Convert pdf -> img\n",
    "                source = Image(filename = fileloc, resolution=300)\n",
    "                # For multi-page PDFs, we only want to keep the first page\n",
    "                images = source.sequence\n",
    "                print(len(images))\n",
    "                img = Image(images[0])\n",
    "                # Settings for saving to PNG\n",
    "                img.background_color = Color(\"white\")\n",
    "                img.alpha_channel = 'remove'\n",
    "                # Save as png to img folder\n",
    "                fin, file_extension = os.path.splitext(row['file'])\n",
    "                #fout = 'data/TrafficCountData/Manual/Images/' + fin + '.png'\n",
    "                fout = 'data/TrafficCountData/Manual/Images/6TH.STANDREWS.120501-MAN.png'\n",
    "                print(fout)\n",
    "                img.save(filename=fout)\n",
    "                img = cv2.imread(fout)\n",
    "                # Run Ped Volume Extract Function\n",
    "                Manual_TC = {}\n",
    "                Manual_TC['Pedestrian'] = GetPedData(img)\n",
    "                if(len(images) > 1):\n",
    "                    print(\"multi-page pdf\")\n",
    "                    print(Manual_TC)\n",
    "                # Append all volumes to new list\n",
    "                for k in Manual_TC['Pedestrian']:\n",
    "                    k['count_id'] = str(count_id)\n",
    "                    ped_volumes.append(k)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Convert dataframes from list of dictionaries and output to csv\n",
    "pedestrian_df = pd.DataFrame.from_records(ped_volumes)\n",
    "print(pedestrian_df)\n",
    "#pedestrian_df.to_csv(path_or_buf='data/TrafficCountData/Results/pedestrian.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-37afd8ad28ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#data.b = pd.to_numeric(data.b,errors='coerce')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mped_df1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mped_df1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mped_df1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mped_df1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'volume'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mped_df2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/TrafficCountData/Results/pedestrian.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# For each df, sum by count_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[0;32m   4984\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4985\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4986\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4987\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4988\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[0;32m   4995\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4996\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[1;32m-> 4997\u001b[1;33m                                          **kwargs)\n\u001b[0m\u001b[0;32m   4998\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   3712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3713\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3714\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'astype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3716\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3581\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[1;32m--> 575\u001b[1;33m                             **kwargs)\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_astype\u001b[1;34m(self, dtype, copy, errors, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                 \u001b[1;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             raise ValueError('Cannot convert non-finite values (NA or inf) to '\n\u001b[0m\u001b[0;32m    703\u001b[0m                              'integer')\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "# Let's compare ped volume totals from both methods\n",
    "\n",
    "# Import original count file\n",
    "ped_df1 = pd.read_csv('data/TrafficCountData/Results/20180323/pedestrian.csv')\n",
    "ped_df1.volume = pd.to_numeric(ped_df1.volume, errors='coerce')\n",
    "ped_df1 = ped_df1.astype({'volume':'int'}, copy=False)\n",
    "ped_df2 = pd.read_csv('data/TrafficCountData/Results/pedestrian.csv')\n",
    "# For each df, sum by count_id\n",
    "ped_df1_total = ped_df1.groupby(['count_id']).sum()\n",
    "ped_df2_total = ped_df2.groupby(['count_id']).sum()\n",
    "# Join by count_id\n",
    "pd_df_total = pd.merge(ped_df2_total, ped_df1_total, how='left', on='count_id')\n",
    "print(len(pd_df_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
