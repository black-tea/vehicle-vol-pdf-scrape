{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping LADOT Volume Data from PDFs, Part 2\n",
    "\n",
    "##### Where I Left Off\n",
    "In the first python notebook, I described the problem and general approach I was taking to solve it. I downloaded all the manual count PDF documents and setup the file tables for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Text Extraction Process\n",
    "As I discussed in the first python notebook, my process for extracting the text into usable data can be broken down into the few key parts: (1) define bounding boxes, (2) search for text within the bounding boxes (3) reformat the resulting text into multiple data tables, and (4) join the resulting tables to the ID established by the Bureau of Engineering. \n",
    "\n",
    "##### Define Bounding Boxes\n",
    "This was tricky. I initially began defining bounding boxes using pixel measurements from a few sample pages. However, I quickly realized that due to the second of the challenges I mentioned above that this would not work, since the tables are in different locations among the PDF documents. \n",
    "\n",
    "Instead, I decided to create bounding boxes on the fly for each document using relative positioning of certain keywords that appeared almost always on each PDF document. Using pdfquery, I could begin by searching the document for these keywords and then extract the x,y pixel coordinate locations for the bounding box of each one. By getting the coordinates of multiple keyword objects on the page, I could construct a set of bounding boxes that seemed to perform relatively well in capturing data tables.\n",
    "\n",
    "(create image of what this looked like)\n",
    "\n",
    "##### Search for Text within Bounding Boxes\n",
    "Once I had the coordinates of the bounding boxes, this part was quite easy, using PDFQuery to extract text\n",
    "\n",
    "(do i need to adjust any of the parameters in rapidminer??)\n",
    "\n",
    "##### Reformat the Resulting Text into Data Tables\n",
    "The final problem included taking the scraped text from the bounding boxes and reformatting them into usable data tables. I kept in mind the relational database model as I set the format for these tables. From the PDF image above, I decided on the following tables and attributes:\n",
    "\n",
    "*tbl_manualcount_info:* This table contains the basic information about the manual count summary. Each count will have one tuple with the following information:\n",
    "* street_ns: The North / South Street running through the intersection\n",
    "* street_ew: The East / West street running through the intersection\n",
    "* dayofweek: Day of the Week\n",
    "* date: Date, in datetime format\n",
    "* weather: Prevailing weather at the time of the count (Clear, Sunny, etc.)\n",
    "* hours: the hours of the count (text)\n",
    "* school_day: A Yes / No indication of whether the count occurred on a school day. This is important because it heavily affects the volume counts\n",
    "* int_code: The \"I/S Code\" on the form corresponds to the CL_Node_ID on the BOE Centerline. This ID field makes it easy to join to the City's centerline network\n",
    "* district: The DOT field district in which the count took place\n",
    "* count_id: Unique identifier assigned to the summary\n",
    "\n",
    "*tbl_manualcount_dualwheel:* This table contains count data for dual-wheeled (motorcycles), bikes, and buses. Each form will have 12 tuples with the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Intersection approach being measured (N,S,E,W)\n",
    "* type: Dual-Wheeled / Bikes / Buses\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_peak:* This table contains the peak hour / 15 minute counts. Each form will have 16 tuples with the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Intersection approach being measured (N,S,E,W)\n",
    "* type: The type of count\n",
    "    * am_15: The AM peak 15 minute count\n",
    "    * am_hour: The AM peak hour count\n",
    "    * pm_15: The PM peak 15 minute count\n",
    "    * pm_hour: The PM peak hour count\n",
    "* time: Time of each count (in datetime format)\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_volumes:* This table contains the main volume counts for each approach at the intersection. The number of tuples for each form will vary depending on the number of hours surveyed. A 6-hour count will have 6 hours * 3 directions (left, right, through) * 4 approach directions = 54 tuples. Each tuple will have the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* approach: Northbound (NB) / Southbound (SB) / Eastbound (EB) / Westbound (WB)\n",
    "* movement: Right-Turn (Rt) / Through (Th) / Left-Turn (Lt)\n",
    "* start_time: Start time of that count, in datetime format\n",
    "* end_time: End time of that count, in datetime format\n",
    "* volume: Count\n",
    "\n",
    "*tbl_manualcount_peds:* This table contains pedestrian and schoolchildren counts during the same time as the main volume counts, so the number of tuples will also be dependent on the number of hours the location was surveyed. Each tuple will have the following information:\n",
    "* count_id: Unique identifier assigned to the summary in \"tbl_manual_count_info\"\n",
    "* xing_leg: The leg of the intersection that is being crossed. South Leg (SL) / North Leg (NL) / West Leg (WL) / East Leg (EL)\n",
    "* type: Ped / Sch\n",
    "* start_time: Start time of that count, in datetime format\n",
    "* end_time: End time of that count, in datetime format\n",
    "* volume: Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime, date, time\n",
    "import pdfquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib2 import Path\n",
    "\n",
    "### Load NavLA Count file table\n",
    "count_files_df = pd.read_csv('data/TrafficCountFileStructure/navLAfiles.csv',index_col=0)\n",
    "\n",
    "count_files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the Data Tables\n",
    "Based on the outline above, the first step is to prepare dataframes for each of the volume-related tables discussed above. Once I have the dataframe table structures, the next step involves looping through all the manual count PDFs, running my text extract function, and then inserting the rows into the appropriate tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Load NavLA Count file table\n",
    "count_files_df = pd.read_csv('data/TrafficCountFileStructure/navLAfiles.csv',index_col=0)\n",
    "manual_count_files_df = count_files_df[(count_files_df['type'] == 'manual')].reset_index()\n",
    "\n",
    "manual_count_files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the PDF text scraping script (extract / transform)\n",
    "The powerhouse behind this process is the script I built to read a PDF and extract text into a list of dictionaries (one for each table). Because this script is so long, I opted not to include the code in the notebook, and instead treated it as a module that I imported using \"import VolumeCountSheets_V2\" above. I call the \"pdf_extract\" function from the module on each PDF.\n",
    "\n",
    "### Step 3: Append each resulting dictionary to the Pandas dataframe (load)\n",
    "The function returns a list of dictionaries, one for each dataframe. I then take the pandas dataframes I constructed in the cell above and append each dictionary to the appropriate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF Scrape Module\n",
    "import VolumeCountSheets_V2\n",
    "import VolumeCountSheets_V3\n",
    "\n",
    "# Setup Counter of sucessful / failed attempts\n",
    "success = 0\n",
    "failures = 0\n",
    "\n",
    "# Create empty lists for storing count data\n",
    "manualcount_volumes = []\n",
    "ped_volumes = []\n",
    "peak_volumes = []\n",
    "info = []\n",
    "specveh_volumes = []\n",
    "qa = []\n",
    "\n",
    "#### Loop through files, Run function\n",
    "for index, row in manual_count_files_df.iterrows():\n",
    "    \n",
    "    Manual_TC = []\n",
    "    \n",
    "     print(row['file'])\n",
    "#     if index > 200:\n",
    "#         break\n",
    "    \n",
    "    # Folder location of all the count PDFs\n",
    "    fileloc = 'data/TrafficCountData/Manual/All/' + row['file']\n",
    "    #fileloc = 'data/TrafficCountData/Manual/All/' + '4347_IMPMON97.pdf'\n",
    "    \n",
    "    # ID for the count\n",
    "    count_id = row['count_id']\n",
    "    if index%100 == 0:\n",
    "        print(\"Current Count:\" + str(index+1))\n",
    "\n",
    "    if Path(fileloc).exists():\n",
    "\n",
    "        # Run the extract function\n",
    "        try:\n",
    "            Manual_TC = VolumeCountSheets_V2.pdf_extract(fileloc)\n",
    "        except:\n",
    "            try:\n",
    "                print('trying v3')\n",
    "                Manual_TC = VolumeCountSheets_V3.pdf_extract(fileloc)\n",
    "            except:\n",
    "                pass\n",
    "                #raise\n",
    "                \n",
    "        # Add data if successful\n",
    "        if len(Manual_TC) > 0:\n",
    "                       \n",
    "            # Append each row to our lists\n",
    "            #if row['file'] == '4347_IMPMON97.PDF':\n",
    "            #print(Manual_TC)\n",
    "            for m in Manual_TC['Spec_Veh']:\n",
    "                m['count_id'] = str(count_id)\n",
    "                specveh_volumes.append(m)\n",
    "            \n",
    "            for j in Manual_TC['Volume']:\n",
    "                j['count_id'] = str(count_id)\n",
    "                manualcount_volumes.append(j)\n",
    "                \n",
    "            for k in Manual_TC['Pedestrian']:\n",
    "                k['count_id'] = str(count_id)\n",
    "                ped_volumes.append(k)\n",
    "                \n",
    "            for l in Manual_TC['PeakVol']:\n",
    "                l['count_id'] = str(count_id)\n",
    "                peak_volumes.append(l)\n",
    "            \n",
    "            Manual_TC['QA']['count_id'] = str(count_id)\n",
    "            qa.append(Manual_TC['QA'])\n",
    "                \n",
    "            Manual_TC['Info']['count_id'] = str(count_id)\n",
    "            info.append(Manual_TC['Info'])\n",
    "            \n",
    "            # Update success counter\n",
    "            success+=1\n",
    "            print('success')\n",
    "               \n",
    "        else:\n",
    "            failures += 1\n",
    "    else:\n",
    "        print('path does not exist')\n",
    "            \n",
    "# Create dataframes from lists of dictionaries\n",
    "info_df = pd.DataFrame.from_records(info)\n",
    "manualcount_df = pd.DataFrame.from_records(manualcount_volumes)\n",
    "pedestrian_df = pd.DataFrame.from_records(ped_volumes)\n",
    "peakvol_df = pd.DataFrame.from_records(peak_volumes)\n",
    "specveh_df = pd.DataFrame.from_records(specveh_volumes)\n",
    "qa_df = pd.DataFrame.from_records(qa)\n",
    "\n",
    "# Save dataframes to CSVs\n",
    "manualcount_df.to_csv(path_or_buf='data/TrafficCountData/Results/manualcount_test.csv',sep=',')\n",
    "pedestrian_df.to_csv(path_or_buf='data/TrafficCountData/Results/pedestrian_test.csv',sep=',')\n",
    "peakvol_df.to_csv(path_or_buf='data/TrafficCountData/Results/peakvol_test.csv',sep=',')\n",
    "specveh_df.to_csv(path_or_buf='data/TrafficCountData/Results/SpecialVehicle_test.csv',sep=',')\n",
    "info_df.to_csv(path_or_buf='data/TrafficCountData/Results/info_test.csv',sep=',')\n",
    "qa_df.to_csv(path_or_buf='data/TrafficCountData/Results/qa_test.csv',sep=',')\n",
    "\n",
    "# Print the dataframes here\n",
    "print(\"Success Count\")\n",
    "print(str(success))\n",
    "print(\"Failure Count\")\n",
    "print(failures)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
